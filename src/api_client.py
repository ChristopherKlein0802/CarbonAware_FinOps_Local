"""
Unified API Client for Carbon-Aware FinOps Dashboard
Pragmatic Professional Implementation - Clean, single-file API integration

Handles ALL external API integrations: Carbon, Power, and AWS Cost data
Bachelor Thesis requirement: NO FALLBACKS - real data only
"""

import os
import json
import requests
import boto3
from datetime import datetime, timedelta
from typing import Optional, Dict, List
import logging
from dataclasses import dataclass
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

logger = logging.getLogger(__name__)

# AWS region to ElectricityMap zone mappings
REGION_MAPPINGS = {
    "eu-central-1": "DE",  # Germany
    "eu-west-1": "IE",     # Ireland
    "eu-west-2": "GB",     # United Kingdom
    "eu-west-3": "FR",     # France
    "eu-north-1": "SE",    # Sweden
    "us-east-1": "US-NE-ISO",    # US East
    "us-west-2": "US-NW-PACW",   # US West
}

@dataclass
class CarbonIntensity:
    """Carbon intensity data structure."""
    value: float  # gCO2/kWh
    timestamp: datetime
    region: str
    source: str

@dataclass
class PowerConsumption:
    """Power consumption data structure for AWS instances."""
    avg_power_watts: float
    min_power_watts: float
    max_power_watts: float
    confidence_level: str
    source: str

@dataclass
class AWSCostData:
    """AWS cost data structure."""
    monthly_cost_usd: float
    service_costs: Dict[str, float]
    region: str
    source: str

def parse_iso_datetime(datetime_str: str) -> datetime:
    """Parse ISO datetime string, handling 'Z' timezone indicator."""
    if datetime_str.endswith("Z"):
        datetime_str = datetime_str[:-1]

    if "+" in datetime_str:
        datetime_str = datetime_str.split("+")[0]
    elif datetime_str.count("-") > 2:
        for i in range(len(datetime_str) - 1, -1, -1):
            if datetime_str[i] in ["-", "+"] and i > 10:
                datetime_str = datetime_str[:i]
                break

    try:
        if "." in datetime_str:
            return datetime.strptime(datetime_str, "%Y-%m-%dT%H:%M:%S.%f")
        else:
            return datetime.strptime(datetime_str, "%Y-%m-%dT%H:%M:%S")
    except ValueError:
        return datetime.now()

class UnifiedAPIClient:
    """
    Unified client for all external APIs - Clean, professional implementation

    Features:
    - ElectricityMap for carbon intensity (30min cache)
    - Boavizta for power consumption (24h cache)
    - AWS Cost Explorer for billing (1h cache)
    - NO FALLBACKS - Academic integrity maintained
    """

    def __init__(self, aws_profile: str = None):
        """Initialize the unified API client"""
        # Load environment variables
        load_dotenv()

        # AWS profile configuration
        if aws_profile is None:
            aws_profile = os.getenv('AWS_PROFILE', 'carbon-finops-sandbox')

        # Initialize clients
        self.aws_profile = aws_profile
        self._init_cache_dir()

        logger.info("‚úÖ Unified API Client initialized")

    def _init_cache_dir(self):
        """Initialize persistent cache directory for cross-session caching"""
        # Use project directory for persistent cache (not /tmp which gets cleared)
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        self.cache_dir = os.path.join(project_root, ".cache", "api_data")
        os.makedirs(self.cache_dir, exist_ok=True)

        # Create .gitignore for cache directory
        gitignore_path = os.path.join(os.path.dirname(self.cache_dir), ".gitignore")
        if not os.path.exists(gitignore_path):
            with open(gitignore_path, "w") as f:
                f.write("# API Cache - exclude from git\n")
                f.write("api_data/\n")
                f.write("*.json\n")

    def _is_cache_valid(self, cache_path: str, max_age_minutes: int) -> bool:
        """Check if cache file is still valid"""
        if not os.path.exists(cache_path):
            return False

        file_age = datetime.now().timestamp() - os.path.getmtime(cache_path)
        return file_age < (max_age_minutes * 60)

    def get_current_carbon_intensity(self, region: str = "eu-central-1") -> Optional[CarbonIntensity]:
        """Get current carbon intensity from ElectricityMap with optimized 2-hour caching

        Scientific rationale: ElectricityMap updates every 15-60 minutes.
        2-hour cache reduces API costs while maintaining reasonable accuracy.
        """
        cache_path = os.path.join(self.cache_dir, f"carbon_intensity_{region}.json")

        # Check cache first (2 hours - optimized for cost vs accuracy)
        if self._is_cache_valid(cache_path, 120):
            try:
                with open(cache_path, "r") as f:
                    cached_data = json.load(f)
                logger.info(f"‚úÖ Using cached carbon data for {region}")
                return CarbonIntensity(
                    value=cached_data["value"],
                    timestamp=parse_iso_datetime(cached_data["timestamp"]),
                    region=cached_data["region"],
                    source=cached_data["source"]
                )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Carbon cache read failed: {e}")

        # Fetch fresh data from ElectricityMap API
        api_key = os.getenv("ELECTRICITYMAP_API_KEY")
        if not api_key:
            logger.error("‚ùå ElectricityMap API key not available - NO FALLBACK used")
            return None

        headers = {"auth-token": api_key}
        zone = REGION_MAPPINGS.get(region, "DE")

        try:
            logger.info(f"üåø Fetching fresh carbon data for {zone}")
            response = requests.get(
                "https://api-access.electricitymaps.com/v3/carbon-intensity/latest",
                headers=headers,
                params={"zone": zone},
                timeout=30
            )
            response.raise_for_status()

            data = response.json()

            if "carbonIntensity" not in data or "datetime" not in data:
                logger.error(f"‚ùå ElectricityMap API returned invalid data")
                return None

            carbon_data = CarbonIntensity(
                value=float(data["carbonIntensity"]),
                timestamp=parse_iso_datetime(data["datetime"]),
                region=region,
                source="electricitymap"
            )

            # Cache the result
            try:
                cache_data = {
                    "value": carbon_data.value,
                    "timestamp": carbon_data.timestamp.isoformat(),
                    "region": carbon_data.region,
                    "source": carbon_data.source
                }
                with open(cache_path, "w") as f:
                    json.dump(cache_data, f)
                logger.info(f"‚úÖ Carbon intensity: {carbon_data.value}g CO2/kWh (cached 30min)")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to cache carbon data: {e}")

            return carbon_data

        except Exception as e:
            logger.error(f"‚ùå ElectricityMap API failed: {e} - NO FALLBACK used")
            return None

    def get_carbon_intensity_24h(self, region: str = "eu-central-1") -> Optional[List[Dict]]:
        """Get 24-hour historical carbon intensity from ElectricityMap with daily caching

        Scientific rationale: Historical data doesn't change, so daily caching is appropriate.
        Uses real API data instead of hard-coded patterns for academic integrity.

        Returns: List of hourly data points with timestamp and carbon intensity
        """
        # Use date-based cache key since historical data doesn't change
        today = datetime.now().strftime("%Y-%m-%d")
        cache_path = os.path.join(self.cache_dir, f"carbon_intensity_24h_{region}_{today}.json")

        # Check cache first (24 hours - historical data doesn't change)
        if self._is_cache_valid(cache_path, 1440):  # 24 hours in minutes
            try:
                with open(cache_path, "r") as f:
                    cached_data = json.load(f)
                logger.info(f"‚úÖ Using cached 24h carbon data for {region}")
                return cached_data["history"]
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è 24h carbon cache read failed: {e}")

        # Fetch fresh 24h historical data from ElectricityMap API
        api_key = os.getenv("ELECTRICITYMAP_API_KEY")
        if not api_key:
            logger.error("‚ùå ElectricityMap API key not available for 24h data - NO FALLBACK used")
            return None

        headers = {"auth-token": api_key}
        zone = REGION_MAPPINGS.get(region, "DE")

        # Calculate time range for past 24 hours
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(hours=24)

        try:
            logger.info(f"üåø Fetching 24h carbon history for {zone}")

            # Debug log the full request
            request_params = {
                "zone": zone,
                "start": start_time.strftime("%Y-%m-%dT%H:%M:%SZ"),
                "end": end_time.strftime("%Y-%m-%dT%H:%M:%SZ"),
                "granularity": "hourly"
            }
            logger.info(f"üìä API Request params: {request_params}")

            response = requests.get(
                "https://api-access.electricitymaps.com/v3/carbon-intensity/past-range",
                headers=headers,
                params=request_params,
                timeout=30
            )

            # Log response status before raising
            logger.info(f"üìä API Response status: {response.status_code}")

            response.raise_for_status()

            data = response.json()

            # Debug: Check what structure we actually get
            logger.info(f"üìä ElectricityMap 24h API response keys: {list(data.keys())}")

            # Show the actual error message for debugging
            if "error" in data and "message" in data:
                logger.info(f"üìä Full API error response: {data}")

            # Check for API error responses first
            if "error" in data and "message" in data:
                logger.error(f"‚ùå ElectricityMap 24h API error: {data['error']} - {data['message']}")
                logger.info("üí° Possible fixes: Check API key permissions, zone parameter, or time range")
                return None

            # Check for different possible response structures
            if "history" in data and isinstance(data["history"], list):
                history_data = data["history"]
            elif "data" in data and isinstance(data["data"], list):
                history_data = data["data"]
            elif isinstance(data, list):
                history_data = data
            else:
                logger.error(f"‚ùå ElectricityMap 24h API returned invalid data structure. Keys: {list(data.keys())}")
                return None

            # Process and validate the historical data
            processed_history = []
            for entry in history_data:
                if "carbonIntensity" in entry and "datetime" in entry:
                    processed_history.append({
                        "carbonIntensity": float(entry["carbonIntensity"]),
                        "datetime": entry["datetime"],
                        "hour": parse_iso_datetime(entry["datetime"]).hour
                    })

            if len(processed_history) < 12:  # Minimum reasonable data points
                logger.warning(f"‚ö†Ô∏è ElectricityMap returned insufficient 24h data: {len(processed_history)} points")
                return None

            # Cache the result with daily expiration
            try:
                cache_data = {
                    "history": processed_history,
                    "cached_at": datetime.now().isoformat(),
                    "data_source": "electricitymap_24h_api",
                    "region": region,
                    "zone": zone
                }
                with open(cache_path, "w") as f:
                    json.dump(cache_data, f)
                logger.info(f"‚úÖ 24h carbon history: {len(processed_history)} data points (cached 24h)")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to cache 24h carbon data: {e}")

            return processed_history

        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå ElectricityMap 24h API failed: {e} - NO FALLBACK used")
            return None

        except Exception as e:
            logger.error(f"‚ùå ElectricityMap 24h API processing failed: {e} - NO FALLBACK used")
            return None

    def get_self_collected_24h_data(self, region: str = "eu-central-1") -> Optional[List[Dict]]:
        """
        Get 24h carbon data from our own hourly collection system

        Scientific approach: Collect real API data every hour to build our own 24h dataset
        This maintains academic integrity - all data points are real ElectricityMaps data
        """
        collection_file = os.path.join(self.cache_dir, f"hourly_collection_{region}.json")

        # Get current carbon data and store it
        self._store_hourly_carbon_data(region, collection_file)

        # Read and return available collected data
        return self._read_collected_hourly_data(collection_file)

    def _store_hourly_carbon_data(self, region: str, collection_file: str):
        """Store current carbon data with timestamp for hourly collection"""
        try:
            current_data = self.get_current_carbon_intensity(region)
            if not current_data:
                return

            current_hour = datetime.now().replace(minute=0, second=0, microsecond=0)

            # Load existing data
            collected_data = []
            if os.path.exists(collection_file):
                with open(collection_file, 'r') as f:
                    try:
                        collected_data = json.load(f)
                    except json.JSONDecodeError:
                        collected_data = []

            # Check if we already have data for this hour
            hour_key = current_hour.isoformat()
            existing_entry = next((entry for entry in collected_data if entry.get('hour_key') == hour_key), None)

            if not existing_entry:
                # Add new hourly data point
                new_entry = {
                    'hour_key': hour_key,
                    'carbonIntensity': current_data.value,
                    'datetime': current_data.timestamp.isoformat(),
                    'hour': current_hour.hour,
                    'collected_at': datetime.now().isoformat(),
                    'source': 'electricitymap_hourly_collection'
                }
                collected_data.append(new_entry)

                # Keep only last 48 hours of data (for 24h + 24h buffer)
                cutoff_time = datetime.now() - timedelta(hours=48)
                collected_data = [
                    entry for entry in collected_data
                    if datetime.fromisoformat(entry['hour_key'].replace('Z', '')) > cutoff_time
                ]

                # Sort by hour_key
                collected_data.sort(key=lambda x: x['hour_key'])

                # Save updated data
                with open(collection_file, 'w') as f:
                    json.dump(collected_data, f, indent=2)

                logger.info(f"üìä Stored hourly carbon data: {current_data.value}g CO‚ÇÇ/kWh for {current_hour.strftime('%H:00')}")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to store hourly carbon data: {e}")

    def _read_collected_hourly_data(self, collection_file: str) -> Optional[List[Dict]]:
        """Read and process collected hourly data for 24h visualization"""
        try:
            if not os.path.exists(collection_file):
                logger.info("üìä No hourly collection data yet - building dataset over time")
                return None

            with open(collection_file, 'r') as f:
                collected_data = json.load(f)

            if not collected_data:
                return None

            # Filter for last 24 hours
            now = datetime.now()
            cutoff_time = now - timedelta(hours=24)

            recent_data = [
                entry for entry in collected_data
                if datetime.fromisoformat(entry['hour_key'].replace('Z', '')) > cutoff_time
            ]

            if len(recent_data) >= 6:  # At least 6 hours of data to show meaningful trend
                logger.info(f"üìä Retrieved {len(recent_data)} hours of self-collected carbon data")
                return recent_data
            else:
                logger.info(f"üìä Only {len(recent_data)} hours collected so far - need more time to build 24h dataset")
                return None

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to read collected hourly data: {e}")
            return None

    def get_power_consumption(self, instance_type: str) -> Optional[PowerConsumption]:
        """Get power consumption from Boavizta API with 7-day caching"""
        if not instance_type:
            logger.error("‚ùå Invalid instance_type")
            return None

        cache_path = os.path.join(self.cache_dir, f"boavizta_power_{instance_type}.json")

        # Check cache first (7 days - hardware specs don't change)
        if self._is_cache_valid(cache_path, 7 * 24 * 60):
            try:
                with open(cache_path, "r") as f:
                    cached_data = json.load(f)
                logger.info(f"‚úÖ Using cached power data for {instance_type}")
                return PowerConsumption(
                    avg_power_watts=cached_data["avg_power_watts"],
                    min_power_watts=cached_data["min_power_watts"],
                    max_power_watts=cached_data["max_power_watts"],
                    confidence_level=cached_data["confidence_level"],
                    source=cached_data["source"]
                )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Power cache read failed: {e}")

        # Fetch fresh data from Boavizta API
        try:
            logger.info(f"‚ö° Fetching fresh power data for {instance_type}")
            url = "https://api.boavizta.org/v1/cloud/instance"
            payload = {
                "provider": "aws",
                "instance_type": instance_type,
                "usage": {"hours_use_time": 1},
                "location": "EUC"  # Europe Central
            }

            headers = {"Content-Type": "application/json", "Accept": "application/json"}
            response = requests.post(url, json=payload, headers=headers, timeout=10)
            response.raise_for_status()

            data = response.json()

            # Extract power consumption from verbose.avg_power
            if "verbose" in data and "avg_power" in data["verbose"]:
                avg_power_data = data["verbose"]["avg_power"]
                if "value" not in avg_power_data:
                    logger.error(f"‚ùå Boavizta API: invalid avg_power structure")
                    return None

                avg_power = avg_power_data.get("value", 0)
                min_power = data["verbose"].get("min_power", {}).get("value", avg_power * 0.8)
                max_power = data["verbose"].get("max_power", {}).get("value", avg_power * 1.2)

                if avg_power <= 0:
                    logger.error(f"‚ùå Boavizta API: invalid power value {avg_power}")
                    return None

                power_data = PowerConsumption(
                    avg_power_watts=float(avg_power),
                    min_power_watts=float(min_power),
                    max_power_watts=float(max_power),
                    confidence_level="high",
                    source="Boavizta_API"
                )

                # Cache the result
                try:
                    cache_data = {
                        "avg_power_watts": power_data.avg_power_watts,
                        "min_power_watts": power_data.min_power_watts,
                        "max_power_watts": power_data.max_power_watts,
                        "confidence_level": power_data.confidence_level,
                        "source": power_data.source
                    }
                    with open(cache_path, "w") as f:
                        json.dump(cache_data, f)
                    logger.info(f"‚úÖ Power: {instance_type} = {avg_power:.1f}W (cached 24h)")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Failed to cache power data: {e}")

                return power_data
            else:
                logger.error(f"‚ùå Boavizta API: missing power data for {instance_type}")
                return None

        except Exception as e:
            logger.error(f"‚ùå Boavizta API failed for {instance_type}: {e} - NO FALLBACK used")
            return None

    def get_instance_pricing(self, instance_type: str, region: str = "eu-central-1") -> Optional[float]:
        """Get AWS EC2 instance pricing from AWS Pricing API with 7-day caching"""
        cache_path = os.path.join(self.cache_dir, f"pricing_{instance_type}_{region}.json")

        # Check cache first (7 days - AWS pricing changes rarely)
        if self._is_cache_valid(cache_path, 7 * 24 * 60):
            try:
                with open(cache_path, "r") as f:
                    cached_data = json.load(f)
                logger.info(f"‚úÖ Using cached pricing for {instance_type}")
                return cached_data["hourly_price_usd"]
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Pricing cache read failed: {e}")

        # Fetch fresh pricing from AWS Pricing API
        try:
            session = boto3.Session(profile_name=self.aws_profile)
            pricing_client = session.client('pricing', region_name='us-east-1')

            # Map region to pricing location
            region_mapping = {
                "eu-central-1": "EU (Frankfurt)",
                "eu-west-1": "EU (Ireland)",
                "us-east-1": "US East (N. Virginia)",
                "us-west-2": "US West (Oregon)"
            }
            location = region_mapping.get(region, "EU (Frankfurt)")

            logger.info(f"üí∞ Fetching fresh pricing for {instance_type} in {location}")

            response = pricing_client.get_products(
                ServiceCode='AmazonEC2',
                Filters=[
                    {'Type': 'TERM_MATCH', 'Field': 'location', 'Value': location},
                    {'Type': 'TERM_MATCH', 'Field': 'instanceType', 'Value': instance_type},
                    {'Type': 'TERM_MATCH', 'Field': 'tenancy', 'Value': 'Shared'},
                    {'Type': 'TERM_MATCH', 'Field': 'operatingSystem', 'Value': 'Linux'},
                    {'Type': 'TERM_MATCH', 'Field': 'preInstalledSw', 'Value': 'NA'},
                    {'Type': 'TERM_MATCH', 'Field': 'capacitystatus', 'Value': 'Used'}
                ]
            )

            if not response.get('PriceList'):
                logger.error(f"‚ùå No pricing data found for {instance_type} in {location}")
                return None

            # Parse the complex pricing JSON structure
            price_item = json.loads(response['PriceList'][0])
            terms = price_item['terms']['OnDemand']

            # Extract the hourly price
            for term_key, term_value in terms.items():
                for price_key, price_value in term_value['priceDimensions'].items():
                    if 'Hrs' in price_value['unit']:
                        hourly_price = float(price_value['pricePerUnit']['USD'])

                        # Cache the result
                        try:
                            cache_data = {
                                "hourly_price_usd": hourly_price,
                                "instance_type": instance_type,
                                "region": region,
                                "location": location,
                                "source": "AWS_Pricing_API"
                            }
                            with open(cache_path, "w") as f:
                                json.dump(cache_data, f)
                            logger.info(f"‚úÖ Pricing: {instance_type} = ${hourly_price:.4f}/hour (cached 24h)")
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Failed to cache pricing data: {e}")

                        return hourly_price

            logger.error(f"‚ùå Could not parse pricing data for {instance_type}")
            return None

        except Exception as e:
            logger.error(f"‚ùå AWS Pricing API failed for {instance_type}: {e} - NO FALLBACK used")
            return None

    def get_monthly_costs(self) -> Optional[AWSCostData]:
        """Get monthly AWS costs for validation with 6-hour caching"""
        cache_path = os.path.join(self.cache_dir, "cost_data.json")

        # Check cache first (6 hours - AWS Cost Explorer updates daily)
        if self._is_cache_valid(cache_path, 6 * 60):
            try:
                with open(cache_path, "r") as f:
                    cached_data = json.load(f)
                logger.info("‚úÖ Using cached Cost Explorer data")
                return AWSCostData(**cached_data)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Cost cache read failed: {e}")

        # Fetch fresh data from AWS Cost Explorer for validation
        try:
            session = boto3.Session(profile_name=self.aws_profile)
            cost_client = session.client("ce", region_name="us-east-1")

            logger.info("üí∞ Fetching Cost Explorer data for validation")

            # Get current month cost data
            end_date = datetime.now().date()
            start_date = end_date.replace(day=1)  # First day of current month

            response = cost_client.get_cost_and_usage(
                TimePeriod={
                    "Start": start_date.strftime("%Y-%m-%d"),
                    "End": end_date.strftime("%Y-%m-%d")
                },
                Granularity="MONTHLY",
                Metrics=["UnblendedCost"],
                GroupBy=[{"Type": "DIMENSION", "Key": "SERVICE"}]
            )

            service_costs = {}
            total_cost = 0.0

            for result in response["ResultsByTime"]:
                for group in result["Groups"]:
                    if group["Keys"]:
                        service_name = group["Keys"][0]
                        cost_amount_str = group["Metrics"]["UnblendedCost"]["Amount"]

                        try:
                            cost_amount = float(cost_amount_str)
                            if cost_amount < 0:
                                cost_amount = 0.0
                        except (ValueError, TypeError):
                            cost_amount = 0.0

                        service_costs[service_name] = cost_amount
                        total_cost += cost_amount

            # Focus on EC2 costs for validation
            ec2_cost = service_costs.get("Amazon Elastic Compute Cloud - Compute", 0.0)

            cost_data = AWSCostData(
                monthly_cost_usd=ec2_cost,  # Only EC2 costs for instance validation
                service_costs=service_costs,
                region="us-east-1",
                source="AWS_Cost_Explorer_Validation"
            )

            # Cache the result
            try:
                cache_data = {
                    "monthly_cost_usd": cost_data.monthly_cost_usd,
                    "service_costs": cost_data.service_costs,
                    "region": cost_data.region,
                    "source": cost_data.source
                }
                with open(cache_path, "w") as f:
                    json.dump(cache_data, f)
                logger.info(f"‚úÖ AWS EC2 costs: ${ec2_cost:.2f} USD (validation data, cached 1h)")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to cache cost data: {e}")

            return cost_data

        except Exception as e:
            logger.error(f"‚ùå AWS Cost Explorer failed: {e} - NO FALLBACK used")
            # Check if it's an SSO token issue
            if "Token has expired and refresh failed" in str(e) or "InvalidGrantException" in str(e):
                logger.warning("üí° AWS SSO token expired for Cost Explorer. Please re-authenticate with 'aws sso login'")
            return None

# Global instance
unified_api_client = UnifiedAPIClient()